# -*- coding: utf-8 -*-
"""MNIST Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yluf53nyPRJ94uO-FFcXtttODVzoRAgt

## **A Detailed View to MNIST Classification**
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from keras import Sequential
from keras.layers import Flatten,Dense
from keras.layers import Dropout

#Load the MNIST data

(X_train, y_train),(X_test,y_test) = keras.datasets.mnist.load_data()

X_train

#To scale the values 

X_train = X_train/255
X_test = X_test/255

"""**1.1 Fully connected net with five hidden layers each of which is with 1024 hidden units**"""

model = Sequential()

Input = model.add(Flatten(input_shape=(28,28)))
Hidden_layer1 = model.add(Dense(1024,activation='relu'))
Hidden_layer2 = model.add(Dense(1024,activation='relu'))
Hidden_layer3 = model.add(Dense(1024,activation='relu'))
Hidden_layer4 = model.add(Dense(1024,activation='relu'))
Hidden_layer5 = model.add(Dense(1024,activation='relu'))
Output = model.add(Dense(10, activation='softmax'))

model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy',metrics = ['accuracy'])

#Training the model

model_train = model.fit(X_train,y_train,batch_size=128,epochs=24,verbose=1,validation_split=0.2)

"""**1.2 Test accuracy result is 98.09%**"""

model.evaluate(X_test,y_test)

model.predict(X_test)

"""**1.3 Feedforward step on 1000 test samples**"""

x_test_2 = X_test[:1000]
y_test_2 = y_test[:1000]

#10-dim probability vector per sample

model.predict(np.expand_dims(x_test_2[0],axis=0)).shape

predictions = np.argmax(model.predict(x_test_2), axis=1)

#1000 predictions 

predictions

prediction_indices = []
for i in range(10):
    prediction_indices.append(np.ndarray.tolist((np.where(predictions == i)[0][:10])))
print(prediction_indices)

#10 * 10 grid of subplots 

num_rows, num_cols = 10, 10
fig, ax = plt.subplots(num_rows, num_cols, figsize=(10,10),gridspec_kw={'wspace':0.01, 'hspace':0.8}, squeeze = True)
for r in range(num_rows):
    for c in range(num_cols):
        ax[r,c].axis("off")
        image_index = prediction_indices[r][c]
        ax[r,c].imshow(x_test_2[image_index])
        ax[r,c].set_title(predictions[image_index])
plt.show()

"""**1.4 Repeating the procedure for second to the last layer output**"""

Hidden_output_model = keras.Model(model.input, model.get_layer(index = 3).output)

Hidden_output = Hidden_output_model.predict(x_test_2)

#1024-dim vector 

Hidden_output.shape

#10 random dimensions 

np.random.seed(10)
ten_dim = np.random.randint(1024,size=10)
print(ten_dim)

layer5_output = Hidden_output[:, ten_dim]
labels = np.argmax( layer5_output , axis= 1)
labels

layer5_output_index = []
for i in range(10):
    layer5_output_index.append(np.where(labels == i)[0][:10])
print(layer5_output_index)

num_rows, num_cols = 10, 10
fig, ax = plt.subplots(num_rows, num_cols, figsize=(10,10),gridspec_kw={'wspace':0.01, 'hspace':0.5}, squeeze = True)
for r in range(num_rows):
    for c in range(num_cols):
        ax[r,c].axis("off")
        if c < len(layer5_output_index[r]):
          image_index = layer5_output_index[r][c]
          ax[r,c].imshow(x_test_2[image_index])
          ax[r,c].set_title(labels[image_index])
plt.show()

"""**In 1.3, the features are correctly classified as we have the softmax output layer where as in 1.4, where we have captured the output from the second to the last layer, we have selected 10 random features out of 1024 dimensional vector and this lead to the output where there are some rows that are empty or not with enough number of images as the random features which we have selected might not be able to classify the images into the required classes. The ideal solution for which the images can be properly classified is better feature seperation in the last hidden layer.**

**1.5 to 1.8, Capturing the output of all the layers and checking out the classification by doing tSNE and PCA on 1000 test samples**
"""

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

layer0_output = np.reshape(x_test_2,(1000,784))
sample_label = np.reshape(y_test_2,(1000,))

#Capturing the output of all the layers

Input_layer = keras.Model(model.input, model.get_layer(index = 0).output)
Hidden_output_layer1 = keras.Model(model.input, model.get_layer(index = 1).output)
Hidden_output_layer2 = keras.Model(model.input, model.get_layer(index = 2).output)
Hidden_output_layer3 = keras.Model(model.input, model.get_layer(index = 3).output)
Hidden_output_layer4 = keras.Model(model.input, model.get_layer(index = 4).output)
Hidden_output_layer5 = keras.Model(model.input, model.get_layer(index = 5).output)
output_layer = keras.Model(model.input, model.get_layer(index = 6).output)

layer_input = Input_layer.predict(x_test_2)
layer1_output = Hidden_output_layer1.predict(x_test_2)
layer2_output = Hidden_output_layer2.predict(x_test_2)
layer3_output = Hidden_output_layer3.predict(x_test_2)
layer4_output = Hidden_output_layer4.predict(x_test_2)
layer_output= output_layer.predict(x_test_2)

#defining PCA for classification

def pca(layer_name):
  scaler = StandardScaler()
  #scaled_data = scaler.fit_transform(layer_name)
  x_pca = PCA(n_components=2).fit_transform(layer_name)
  #x_pca = PCA.fit_transform(scaled_data)
  fig= plt.figure(figsize=(10,10))
  figure=plt.scatter(x_pca[:, 0], x_pca[:, 1], c=sample_label, cmap='plasma')
  for i in range(10):
    x_pca0_mean = np.mean(np.take(x_pca[:, 0], np.where(sample_label == i)))
    x_pca1_mean = np.mean(np.take(x_pca[:, 1], np.where(sample_label == i)))
    plt.text(x_pca0_mean, x_pca1_mean, i, fontsize = 12, bbox=dict(boxstyle="round"))
    plt.legend(figure.legend_elements()[0], figure.legend_elements()[1])
  plt.show()

pca(layer_input)

pca(layer1_output)

pca(layer2_output)

pca(layer3_output)

pca(layer4_output)

pca(layer_output)

"""**From the above plots, it is clear that PCA is not able to properly classify the 10 classes.**"""

#defining tSNE for classification

def tsne(layer_name):
  scaler = StandardScaler()
  x_tsne = TSNE(n_components=2).fit_transform(layer_name)
  fig= plt.figure(figsize=(10,10))
  figure=plt.scatter(x_tsne[:, 0], x_tsne[:, 1], c=sample_label, cmap='plasma')
  for i in range(10):
    x_pca0_mean = np.mean(np.take(x_tsne[:, 0], np.where(sample_label == i)))
    x_pca1_mean = np.mean(np.take(x_tsne[:, 1], np.where(sample_label == i)))
    plt.text(x_pca0_mean, x_pca1_mean, i, fontsize = 12, bbox=dict(boxstyle="round"))
    plt.legend(figure.legend_elements()[0], figure.legend_elements()[1])
  plt.show()

tsne(layer_input)

tsne(layer1_output)

tsne(layer2_output)

tsne(layer3_output)

tsne(layer4_output)

tsne(layer_output)